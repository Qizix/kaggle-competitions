{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:32.501634Z",
     "iopub.status.busy": "2024-12-14T07:44:32.500844Z",
     "iopub.status.idle": "2024-12-14T07:44:32.511139Z",
     "shell.execute_reply": "2024-12-14T07:44:32.509868Z",
     "shell.execute_reply.started": "2024-12-14T07:44:32.501593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:32.513488Z",
     "iopub.status.busy": "2024-12-14T07:44:32.513140Z",
     "iopub.status.idle": "2024-12-14T07:44:32.734183Z",
     "shell.execute_reply": "2024-12-14T07:44:32.733084Z",
     "shell.execute_reply.started": "2024-12-14T07:44:32.513454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dd = pd.read_csv('data/data_dictionary.csv')\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:32.735826Z",
     "iopub.status.busy": "2024-12-14T07:44:32.735432Z",
     "iopub.status.idle": "2024-12-14T07:44:32.750363Z",
     "shell.execute_reply": "2024-12-14T07:44:32.749277Z",
     "shell.execute_reply.started": "2024-12-14T07:44:32.735785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(['efs', 'efs_time'], axis=1)\n",
    "y = train[['efs', 'efs_time', 'race_group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:32.752595Z",
     "iopub.status.busy": "2024-12-14T07:44:32.752176Z",
     "iopub.status.idle": "2024-12-14T07:44:32.877965Z",
     "shell.execute_reply": "2024-12-14T07:44:32.876831Z",
     "shell.execute_reply.started": "2024-12-14T07:44:32.752550Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([X, test])\n",
    "\n",
    "'''\n",
    "# many missing data\n",
    "df.drop('tce_match', axis=1, inplace=True)\n",
    "\n",
    "# self encode !!\n",
    "df['mrd_hct'] = df['mrd_hct'].fillna('Unknown')\n",
    "df['mrd_hct'] = df['mrd_hct'].map({'Negative': 0, 'Positive': 1, 'Unknown': -1})\n",
    "\n",
    "# Fill missing values with the mode within `dri_score`\n",
    "df['cyto_score_detail'] = df.groupby('dri_score')['cyto_score_detail'] \\\n",
    "                            .transform(lambda group: group.fillna(group.mode()[0] if not group.mode().empty else 'Unknown'))\n",
    "\n",
    "'''\n",
    "\n",
    "categorical_cols = ['dri_score', 'cyto_score', 'diabetes', 'arrhythmia', 'cmv_status', 'rituximab', 'obesity', \n",
    "                    'in_vivo_tcd', 'tce_match', 'graft_type', 'renal_issue', 'pulm_severe', 'prim_disease_hct', \n",
    "                    'ethnicity', 'conditioning_intensity', 'mrd_hct', 'hla_match_c_high', 'hla_match_b_low', \n",
    "                    'peptic_ulcer', 'prior_tumor', 'hepatic_severe', 'sex_match', 'gvhd_proph', 'rheum_issue', \n",
    "                    'hla_match_b_high', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related']\n",
    "\n",
    "# Ensure columns are numeric before applying median\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# For categorical columns, fill with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "'''for col in categorical_cols:\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "'''\n",
    "# If 'year_hct' or 'age_at_hct' is missing, you could fill with median value\n",
    "df['age_at_hct'].fillna(df['age_at_hct'].median(), inplace=True)\n",
    "df['year_hct'].fillna(df['year_hct'].mode()[0], inplace=True)  # Use mode for most frequent year\n",
    "\n",
    "# For categorical columns where a specific value indicates missing data\n",
    "df['dri_score'].fillna('Missing disease status', inplace=True)\n",
    "df['cyto_score'].fillna('Not tested', inplace=True)\n",
    "df['diabetes'].fillna('Not done', inplace=True)\n",
    "\n",
    "\n",
    "print(\"Check for missing data in train set:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:32.881795Z",
     "iopub.status.busy": "2024-12-14T07:44:32.881348Z",
     "iopub.status.idle": "2024-12-14T07:44:34.785063Z",
     "shell.execute_reply": "2024-12-14T07:44:34.783969Z",
     "shell.execute_reply.started": "2024-12-14T07:44:32.881748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "corr_matrix = pd.DataFrame(df.select_dtypes(include=['number']).corr())\n",
    "\n",
    "plt.figure(figsize=(15, 13)) \n",
    "sns.heatmap(data=corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:34.786540Z",
     "iopub.status.busy": "2024-12-14T07:44:34.786222Z",
     "iopub.status.idle": "2024-12-14T07:44:35.485801Z",
     "shell.execute_reply": "2024-12-14T07:44:35.484741Z",
     "shell.execute_reply.started": "2024-12-14T07:44:34.786509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering: Count missing values per row\n",
    "df['missing_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "# Feature Engineering: Combine HLA match scores into a single metric\n",
    "hla_cols = [\n",
    "    'hla_high_res_6', 'hla_high_res_8', 'hla_high_res_10',\n",
    "    'hla_match_a_high', 'hla_match_b_high', 'hla_match_c_high',\n",
    "    'hla_match_dqb1_high', 'hla_low_res_6', 'hla_low_res_8'\n",
    "]\n",
    "df['hla_match_avg'] = df[hla_cols].mean(axis=1)\n",
    "\n",
    "# Feature Engineering: Binary feature for high-risk DRI scores\n",
    "df['dri_high_risk'] = df['dri_score'].apply(\n",
    "    lambda x: 1 if isinstance(x, str) and ('High' in x or 'Very high' in x) else 0\n",
    ")\n",
    "\n",
    "# Feature Engineering: Binary feature for any psychiatric disturbance\n",
    "df['psych_disturb_binary'] = df['psych_disturb'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Feature Engineering: Create a composite comorbidity score\n",
    "comorbidity_cols = [\n",
    "    'cardiac', 'renal_issue', 'hepatic_severe', 'pulm_severe', \n",
    "    'rheum_issue', 'diabetes', 'obesity', 'vent_hist', 'arrhythmia'\n",
    "]\n",
    "df['comorbidity_severity'] = df[comorbidity_cols].apply(\n",
    "    lambda row: sum([1 for val in row if val == 'Yes']), axis=1\n",
    ")\n",
    "\n",
    "# Feature Engineering: Binary feature for severe conditions\n",
    "df['severe_conditions'] = df[['hepatic_severe', 'pulm_severe', 'renal_issue']].apply(\n",
    "    lambda row: 1 if 'Yes' in row.values else 0, axis=1\n",
    ")\n",
    "\n",
    "# Feature Engineering: Encode donor-recipient sex match as numerical categories\n",
    "sex_match_mapping = {'M-M': 0, 'F-F': 1, 'M-F': 2, 'F-M': 3, np.nan: -1}\n",
    "df['sex_match_encoded'] = df['sex_match'].map(sex_match_mapping)\n",
    "\n",
    "# Feature Engineering: Create interaction features\n",
    "df['hla_cyto_interaction'] = df['hla_match_c_high'] * df['cyto_score'].apply(\n",
    "    lambda x: 1 if x == 'Favorable' else 0\n",
    ")\n",
    "\n",
    "# Feature Engineering: Extract year from 'year_hct' and categorize into decades\n",
    "df['year_hct_decade'] = df['year_hct'].apply(lambda x: (x // 10) * 10 if not pd.isnull(x) else np.nan)\n",
    "\n",
    "# Feature Engineering: Add a risk index based on comorbidity and DRI\n",
    "df['risk_index'] = df['comorbidity_score'] + df['dri_high_risk']\n",
    "\n",
    "# Feature Engineering: Binarize CMV status\n",
    "df['cmv_positive'] = df['cmv_status'].apply(lambda x: 1 if x in ['+/-', '+/+'] else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:35.487412Z",
     "iopub.status.busy": "2024-12-14T07:44:35.487101Z",
     "iopub.status.idle": "2024-12-14T07:44:35.867182Z",
     "shell.execute_reply": "2024-12-14T07:44:35.865914Z",
     "shell.execute_reply.started": "2024-12-14T07:44:35.487381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "object_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # Updated sparse parameter\n",
    "encoded_array = encoder.fit_transform(df[object_cols])\n",
    "\n",
    "# Create a DataFrame for the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(object_cols), index=df.index)\n",
    "\n",
    "# Remove original object columns and add the encoded ones\n",
    "df = pd.concat([df.drop(columns=object_cols), encoded_df], axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example DataFrame (replace `X` with your DataFrame)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # Store the encoder for future use (e.g., inverse_transform)\n",
    "\n",
    "# Check if all columns are now numeric\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:35.910206Z",
     "iopub.status.busy": "2024-12-14T07:44:35.909745Z",
     "iopub.status.idle": "2024-12-14T07:44:35.917732Z",
     "shell.execute_reply": "2024-12-14T07:44:35.916554Z",
     "shell.execute_reply.started": "2024-12-14T07:44:35.910158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df[:-3]\n",
    "X_test = df[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Different columns between X and test:\")\n",
    "print(X.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Kaplan-Meier estimation\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(y['efs_time'], event_observed=y['efs'])\n",
    "\n",
    "y['y'] = kmf.survival_function_at_times(y['efs_time']).values\n",
    "# Plot survival curve\n",
    "kmf.plot_survival_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T07:44:35.920416Z",
     "iopub.status.busy": "2024-12-14T07:44:35.919378Z",
     "iopub.status.idle": "2024-12-14T07:44:35.961351Z",
     "shell.execute_reply": "2024-12-14T07:44:35.960319Z",
     "shell.execute_reply.started": "2024-12-14T07:44:35.920378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rdc = XGBRegressor(random_state=42,\n",
    "                   enable_categorical=True)\n",
    "rdc.fit(X_train, y_train['y'])\n",
    "\n",
    "rdc.score(X_val, y_val['y'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preds = rdc.predict(X_val)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fe: 0.5831621880666985,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from scipy.stats import rankdata \n",
    "preds = rankdata(preds)\n",
    "preds'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# kfold cross-validation\n",
    "from metric import score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from functools import partial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "models = {\n",
    "    # 1. Linear Regression: Default settings\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "\n",
    "    # 2. Decision Tree Regressor: Simple tree with a small depth\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(\n",
    "        max_depth=5, \n",
    "        min_samples_split=10, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 3. Random Forest Regressor: Ensemble with a few trees\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=7, \n",
    "        min_samples_split=10, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 4. Gradient Boosting Regressor: Small learning rate and estimators\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=4, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 5. XGBoost Regressor: Commonly used settings\n",
    "    \"XGBoost Regressor\": XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=4, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 6. LightGBM Regressor: Fast and efficient gradient boosting\n",
    "    \"LightGBM Regressor\": LGBMRegressor(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=-1, \n",
    "        num_leaves=31, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 7. CatBoost Regressor: Fast and powerful gradient boosting\n",
    "    \"CatBoost Regressor\": CatBoostRegressor(\n",
    "        iterations=100, \n",
    "        learning_rate=0.1, \n",
    "        depth=6, \n",
    "        verbose=0, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 8. SVR (Support Vector Regressor): Default kernel and parameters\n",
    "    \"SVR (Support Vector Regressor)\": SVR(\n",
    "        kernel='rbf', \n",
    "        C=1.0, \n",
    "        epsilon=0.1\n",
    "    ),\n",
    "\n",
    "    # 9. Gaussian Process Regressor: Default kernel\n",
    "    \"Gaussian Process Regressor\": GaussianProcessRegressor(\n",
    "        alpha=1e-10, \n",
    "        normalize_y=True\n",
    "    ),\n",
    "\n",
    "    # 10. SGD Regressor: Stochastic Gradient Descent for regression\n",
    "    \"SGD Regressor\": SGDRegressor(\n",
    "        max_iter=1000, \n",
    "        tol=1e-3, \n",
    "        learning_rate='invscaling', \n",
    "        eta0=0.01, \n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "scores = {\n",
    "    \"Linear Regression\": [],\n",
    "    \"Decision Tree Regressor\": [],\n",
    "    \"Random Forest Regressor\": [],\n",
    "    \"Gradient Boosting Regressor\": [],\n",
    "    \"XGBoost Regressor\": [],\n",
    "    \"LightGBM Regressor\": [],\n",
    "    \"CatBoost Regressor\": [],\n",
    "    \"SVR (Support Vector Regressor)\": [],\n",
    "    \"Gaussian Process Regressor\": [],\n",
    "    \"SGD Regressor\": [],\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        model.fit(X.iloc[train_index], y.iloc[train_index]['y'])\n",
    "        \n",
    "        preds = pd.DataFrame({\n",
    "            'prediction': model.predict(X.iloc[test_index]),\n",
    "            'ID': X.iloc[test_index]['ID']\n",
    "        })\n",
    "             \n",
    "        y_true = y.iloc[test_index][[\"efs\", \"efs_time\",\"race_group\"]]\n",
    "        y_true['ID'] = X.iloc[test_index]['ID']\n",
    "\n",
    "        scores[name].append(score(y_true.copy(), preds.copy(), \"ID\"))\n",
    "\n",
    "        \n",
    "    print(f\"{name}: {scores[name]}, Mean: {np.mean(scores[name])}\")\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# kfold cross-validation\n",
    "\n",
    "'''\n",
    "Linear Regression                134 ---\n",
    "Decision Tree Regressor          119 ---\n",
    "Random Forest Regressor          142 ---\n",
    "Gradient Boosting Regressor      161\n",
    "XGBoost Regressor                161\n",
    "LightGBM Regressor               163\n",
    "CatBoost Regressor               161\n",
    "SVR (Support Vector Regressor)   077 ---\n",
    "Gaussian Process Regressor       000 ---\n",
    "SGD Regressor                    013 ---\n",
    "'''\n",
    "\n",
    "\n",
    "from metric import score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from functools import partial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "models = {\n",
    "    # 1. Linear Regression: Default settings\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "\n",
    "    # 2. Decision Tree Regressor: Simple tree with a small depth\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(\n",
    "        max_depth=5, \n",
    "        min_samples_split=10, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 3. Random Forest Regressor: Ensemble with a few trees\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=7, \n",
    "        min_samples_split=10, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 4. Gradient Boosting Regressor: Small learning rate and estimators\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=4, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 5. XGBoost Regressor: Commonly used settings\n",
    "    \"XGBoost Regressor\": XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=4, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 6. LightGBM Regressor: Fast and efficient gradient boosting\n",
    "    \"LightGBM Regressor\": LGBMRegressor(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=-1, \n",
    "        num_leaves=31, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 7. CatBoost Regressor: Fast and powerful gradient boosting\n",
    "    \"CatBoost Regressor\": CatBoostRegressor(\n",
    "        iterations=100, \n",
    "        learning_rate=0.1, \n",
    "        depth=6, \n",
    "        verbose=0, \n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    # 8. SVR (Support Vector Regressor): Default kernel and parameters\n",
    "    \"SVR (Support Vector Regressor)\": SVR(\n",
    "        kernel='rbf', \n",
    "        C=1.0, \n",
    "        epsilon=0.1\n",
    "    ),\n",
    "\n",
    "    # 9. Gaussian Process Regressor: Default kernel\n",
    "    \"Gaussian Process Regressor\": GaussianProcessRegressor(\n",
    "        alpha=1e-10, \n",
    "        normalize_y=True\n",
    "    ),\n",
    "\n",
    "    # 10. SGD Regressor: Stochastic Gradient Descent for regression\n",
    "    \"SGD Regressor\": SGDRegressor(\n",
    "        max_iter=1000, \n",
    "        tol=1e-3, \n",
    "        learning_rate='invscaling', \n",
    "        eta0=0.01, \n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "scores = {\n",
    "    \"Linear Regression\": [],\n",
    "    \"Decision Tree Regressor\": [],\n",
    "    \"Random Forest Regressor\": [],\n",
    "    \"Gradient Boosting Regressor\": [],\n",
    "    \"XGBoost Regressor\": [],\n",
    "    \"LightGBM Regressor\": [],\n",
    "    \"CatBoost Regressor\": [],\n",
    "    \"SVR (Support Vector Regressor)\": [],\n",
    "    \"Gaussian Process Regressor\": [],\n",
    "    \"SGD Regressor\": [],\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        model.fit(X.iloc[train_index], y.iloc[train_index]['y'])\n",
    "        \n",
    "        preds = pd.DataFrame({\n",
    "            'prediction': model.predict(X.iloc[test_index]),\n",
    "            'ID': X.iloc[test_index]['ID']\n",
    "        })\n",
    "             \n",
    "        y_true = y.iloc[test_index][[\"efs\", \"efs_time\",\"race_group\"]]\n",
    "        y_true['ID'] = X.iloc[test_index]['ID']\n",
    "\n",
    "        scores[name].append(score(y_true.copy(), preds.copy(), \"ID\"))\n",
    "\n",
    "        \n",
    "    print(f\"{name}: {scores[name]}, Mean: {np.mean(scores[name])}\")\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_scorer\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Custom scorer to use additional columns in y_true\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom scoring function that uses additional columns from y_true.\n",
    "    \"\"\"\n",
    "    # Extract additional columns ['efs', 'efs_time', 'race_group', 'ID']\n",
    "    additional_cols = y_true[['efs', 'efs_time', 'race_group', 'ID']]\n",
    "\n",
    "    # Prepare predictions DataFrame with 'ID'\n",
    "    preds = pd.DataFrame({\n",
    "        'prediction': y_pred,\n",
    "        'ID': additional_cols['ID']\n",
    "    })\n",
    "\n",
    "    # Call the custom metric function\n",
    "    return score(additional_cols, preds, \"ID\")\n",
    "\n",
    "# Wrap the custom scorer using make_scorer\n",
    "scorer = make_scorer(custom_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# CustomRandomizedSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone, BaseEstimator\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "\n",
    "class CustomRandomizedSearchCV:\n",
    "    def __init__(self, estimator, param_distributions, n_iter=10, cv=None, scoring=None, random_state=None, verbose=2):\n",
    "        self.estimator = estimator\n",
    "        self.param_distributions = param_distributions\n",
    "        self.n_iter = n_iter\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.results_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        sampled_params = list(ParameterSampler(self.param_distributions, self.n_iter, random_state=self.random_state))\n",
    "        \n",
    "        # Progress bar\n",
    "        if self.verbose:\n",
    "            print(f\"Evaluating {self.n_iter} parameter combinations...\")\n",
    "        \n",
    "        for params in tqdm(sampled_params, disable=not self.verbose):\n",
    "            scores = []\n",
    "            \n",
    "            # Cross-validation logic\n",
    "            oof_predictions = np.zeros(len(X))  # Array to store OOF predictions\n",
    "\n",
    "            for train_idx, test_idx in self.cv.split(X, y):\n",
    "                # Split data into training and validation sets\n",
    "                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "                # Clone the estimator and set parameters\n",
    "                model = clone(self.estimator)\n",
    "                model.set_params(**params)\n",
    "                model.fit(X_train, y_train['y'])  # Train only on target y['y']\n",
    "                oof_predictions[test_idx] = model.predict(X_test)  # Save predictions for validation set\n",
    "\n",
    "            y_true = y[[\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "            y_true[['ID']] = X[[\"ID\"]]\n",
    "            y_pred = X[[\"ID\"]].copy()\n",
    "            y_pred[\"prediction\"] = oof_predictions\n",
    "            m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "\n",
    "            self.results_.append({'params': params, 'score': m})\n",
    "        \n",
    "        # Select best result\n",
    "        self.results_ = sorted(self.results_, key=lambda x: x['score'], reverse=True)\n",
    "        self.best_params_ = self.results_[0]['params']\n",
    "        self.best_score_ = self.results_[0]['score']\n",
    "        self.best_estimator_ = clone(self.estimator).set_params(**self.best_params_)\n",
    "        self.best_estimator_.fit(X, y['y'])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"\n",
    "        Return the search results.\n",
    "\n",
    "        Returns:\n",
    "        - results: List of dictionaries with params, scores, and mean_score.\n",
    "        \"\"\"\n",
    "        return self.results_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# GradientBoostingRegressor\\nfrom scipy.stats import randint, uniform\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.model_selection import KFold\\n\\n# Define the parameter grid for RandomizedSearchCV\\nparam_distributions = {\\n    \\'n_estimators\\': randint(50, 3000),         # Number of trees\\n    \\'learning_rate\\': uniform(0.01, 0.2),     # Shrinkage rate\\n    \\'max_depth\\': randint(3, 10),             # Maximum depth of each tree\\n    \\'min_samples_split\\': randint(2, 20),     # Minimum number of samples to split a node\\n    \\'min_samples_leaf\\': randint(1, 20),      # Minimum samples per leaf node\\n    \\'subsample\\': uniform(0.5, 0.5),          # Fraction of samples used for fitting each tree\\n    \\'max_features\\': [\\'sqrt\\', \\'log2\\', None],  # Number of features considered for best split\\n}\\n\\n# Define cross-validation strategy\\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\\n\\n# Initialize the GradientBoostingRegressor model\\nmodel = GradientBoostingRegressor()\\n\\n# Initialize the CustomScorerCVWrapper\\nwrapper = CustomRandomizedSearchCV(\\n    estimator=model,\\n    param_distributions=param_distributions,\\n    scoring=scorer,\\n    cv=kf,\\n    n_iter=100,\\n    random_state=42\\n)\\n\\n# Fit the wrapper with X and y_full\\nwrapper.fit(X, y)\\n\\nprint(wrapper)\\n# Print the best parameters and score\\nprint(\"Best Parameters:\", wrapper.best_params_)\\nprint(\"Best Score:\", wrapper.best_score_)\\nprint(\"All Results:\", wrapper.get_results())'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# GradientBoostingRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 3000),         # Number of trees\n",
    "    'learning_rate': uniform(0.01, 0.2),     # Shrinkage rate\n",
    "    'max_depth': randint(3, 10),             # Maximum depth of each tree\n",
    "    'min_samples_split': randint(2, 20),     # Minimum number of samples to split a node\n",
    "    'min_samples_leaf': randint(1, 20),      # Minimum samples per leaf node\n",
    "    'subsample': uniform(0.5, 0.5),          # Fraction of samples used for fitting each tree\n",
    "    'max_features': ['sqrt', 'log2', None],  # Number of features considered for best split\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the GradientBoostingRegressor model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize the CustomScorerCVWrapper\n",
    "wrapper = CustomRandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=scorer,\n",
    "    cv=kf,\n",
    "    n_iter=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the wrapper with X and y_full\n",
    "wrapper.fit(X, y)\n",
    "\n",
    "print(wrapper)\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", wrapper.best_params_)\n",
    "print(\"Best Score:\", wrapper.best_score_)\n",
    "print(\"All Results:\", wrapper.get_results())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# XGBoost Regressor\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(1000, 3000),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 40),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 1),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1),\n",
    "    'scale_pos_weight': uniform(1, 10),\n",
    "}\n",
    "\n",
    "# Initialize XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(random_state=42)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Set up RandomizedSearchCV with 50 iterations and KFold cross-validation\n",
    "wrapperxgb = CustomRandomizedSearchCV(\n",
    "    estimator=xgb_regressor,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=scorer,\n",
    "    cv=kf,\n",
    "    n_iter=150,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the wrapper with X and y_full\n",
    "wrapperxgb.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", wrapperxgb.best_params_)\n",
    "print(\"Best Score:\", wrapperxgb.best_score_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17778\n",
      "[500]\tvalidation_0-rmse:0.16049\n",
      "[1000]\tvalidation_0-rmse:0.15827\n",
      "[1500]\tvalidation_0-rmse:0.15749\n",
      "[2000]\tvalidation_0-rmse:0.15692\n",
      "[2499]\tvalidation_0-rmse:0.15673\n",
      "#########################\n",
      "### Fold 2\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17357\n",
      "[500]\tvalidation_0-rmse:0.15628\n",
      "[1000]\tvalidation_0-rmse:0.15467\n",
      "[1500]\tvalidation_0-rmse:0.15406\n",
      "[2000]\tvalidation_0-rmse:0.15374\n",
      "[2499]\tvalidation_0-rmse:0.15358\n",
      "#########################\n",
      "### Fold 3\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17731\n",
      "[500]\tvalidation_0-rmse:0.15853\n",
      "[1000]\tvalidation_0-rmse:0.15608\n",
      "[1500]\tvalidation_0-rmse:0.15521\n",
      "[2000]\tvalidation_0-rmse:0.15477\n",
      "[2499]\tvalidation_0-rmse:0.15456\n",
      "#########################\n",
      "### Fold 4\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17928\n",
      "[500]\tvalidation_0-rmse:0.16074\n",
      "[1000]\tvalidation_0-rmse:0.15850\n",
      "[1500]\tvalidation_0-rmse:0.15773\n",
      "[2000]\tvalidation_0-rmse:0.15745\n",
      "[2499]\tvalidation_0-rmse:0.15731\n",
      "#########################\n",
      "### Fold 5\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17372\n",
      "[500]\tvalidation_0-rmse:0.15762\n",
      "[1000]\tvalidation_0-rmse:0.15578\n",
      "[1500]\tvalidation_0-rmse:0.15517\n",
      "[2000]\tvalidation_0-rmse:0.15489\n",
      "[2499]\tvalidation_0-rmse:0.15468\n",
      "#########################\n",
      "### Fold 6\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17756\n",
      "[500]\tvalidation_0-rmse:0.15997\n",
      "[1000]\tvalidation_0-rmse:0.15820\n",
      "[1500]\tvalidation_0-rmse:0.15759\n",
      "[2000]\tvalidation_0-rmse:0.15730\n",
      "[2499]\tvalidation_0-rmse:0.15711\n",
      "#########################\n",
      "### Fold 7\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17855\n",
      "[500]\tvalidation_0-rmse:0.16198\n",
      "[1000]\tvalidation_0-rmse:0.16014\n",
      "[1500]\tvalidation_0-rmse:0.15949\n",
      "[2000]\tvalidation_0-rmse:0.15915\n",
      "[2499]\tvalidation_0-rmse:0.15901\n",
      "#########################\n",
      "### Fold 8\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17462\n",
      "[500]\tvalidation_0-rmse:0.15859\n",
      "[1000]\tvalidation_0-rmse:0.15687\n",
      "[1500]\tvalidation_0-rmse:0.15604\n",
      "[2000]\tvalidation_0-rmse:0.15559\n",
      "[2499]\tvalidation_0-rmse:0.15540\n",
      "#########################\n",
      "### Fold 9\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17654\n",
      "[500]\tvalidation_0-rmse:0.16116\n",
      "[1000]\tvalidation_0-rmse:0.15933\n",
      "[1500]\tvalidation_0-rmse:0.15859\n",
      "[2000]\tvalidation_0-rmse:0.15812\n",
      "[2499]\tvalidation_0-rmse:0.15788\n",
      "#########################\n",
      "### Fold 10\n",
      "#########################\n",
      "[0]\tvalidation_0-rmse:0.17533\n",
      "[500]\tvalidation_0-rmse:0.15903\n",
      "[1000]\tvalidation_0-rmse:0.15724\n",
      "[1500]\tvalidation_0-rmse:0.15651\n",
      "[2000]\tvalidation_0-rmse:0.15620\n",
      "[2499]\tvalidation_0-rmse:0.15608\n",
      "CPU times: total: 1min 34s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''xgbparams = {'colsample_bytree': 0.7, \n",
    "             'learning_rate': 0.02, \n",
    "             'max_depth': 5, \n",
    "             'min_child_weight': 10, \n",
    "             'n_estimators': 2500, \n",
    "             'reg_alpha': 0.7,  \n",
    "             'scale_pos_weight': 5, \n",
    "             'subsample': 0.7,\n",
    "             'enable_categorical':True}'''\n",
    "\n",
    "xgb_regressor.set_params(**xgbparams)\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "        random_state=42,\n",
    "        device=\"cuda\",\n",
    "        max_depth=3,  \n",
    "        colsample_bytree=0.5,  \n",
    "        subsample=0.7,  \n",
    "        n_estimators=2500,  \n",
    "        learning_rate=0.02,  \n",
    "        enable_categorical=True,\n",
    "        min_child_weight=80,\n",
    "        #early_stopping_rounds=25,\n",
    "    )\n",
    "\n",
    "FOLDS = 10\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    \n",
    "oof_xgb = np.zeros(len(X))\n",
    "pred_xgb = np.zeros(len(X_test))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {i+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \n",
    "    xgb_regressor.fit(\n",
    "        X_train, y_train['y'],\n",
    "        eval_set=[(X_valid, y_valid['y'])],  \n",
    "        verbose=500 \n",
    "    )\n",
    "\n",
    "    # INFER OOF\n",
    "    oof_xgb[test_index] = xgb_regressor.predict(X_valid)\n",
    "    # INFER TEST\n",
    "    pred_xgb += xgb_regressor.predict(X_test)\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_xgb /= FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for XGBoost KaplanMeier = 0.67069,  0.6709587632859836\n"
     ]
    }
   ],
   "source": [
    "y_true = y[[\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_true[['ID']] = X[[\"ID\"]]\n",
    "y_pred = X[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = oof_xgb\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nOverall CV for XGBoost KaplanMeier = 0.67095, \",m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Regressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(1000, 3000),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'num_leaves': randint(31, 255),\n",
    "    'min_child_samples': randint(10, 100),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1),\n",
    "    'scale_pos_weight': uniform(1, 10),\n",
    "    'min_split_gain': uniform(0, 1),\n",
    "    'device': [\"gpu\"],\n",
    "    'verbose':[-1], \n",
    "    'objective':[\"regression\"], \n",
    "    \n",
    "}\n",
    "\n",
    "# Initialize LightGBM Regressor\n",
    "lgb_regressor = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV with 50 iterations and KFold cross-validation\n",
    "wrapperLGBM = CustomRandomizedSearchCV(\n",
    "    estimator=lgb_regressor,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=scorer,\n",
    "    cv=kf,\n",
    "    n_iter=150,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the wrapper with X and y_full\n",
    "wrapperLGBM.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", wrapperLGBM.best_params_)\n",
    "print(\"Best Score:\", wrapperLGBM.best_score_)\n",
    "print(\"All Results:\", wrapperLGBM.get_results())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>device</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_samples</th>\n",
       "      <th>min_split_gain</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>num_leaves</th>\n",
       "      <th>objective</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>verbose</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.749816</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.295214</td>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>2638</td>\n",
       "      <td>152</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.155995</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>9.661761</td>\n",
       "      <td>0.840446</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.883229</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>4</td>\n",
       "      <td>97</td>\n",
       "      <td>0.832443</td>\n",
       "      <td>1805</td>\n",
       "      <td>160</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.181825</td>\n",
       "      <td>0.183405</td>\n",
       "      <td>4.042422</td>\n",
       "      <td>0.809903</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.772778</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.097369</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>1699</td>\n",
       "      <td>238</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.232771</td>\n",
       "      <td>0.090606</td>\n",
       "      <td>7.183860</td>\n",
       "      <td>0.752985</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993292</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.150029</td>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "      <td>0.680308</td>\n",
       "      <td>1840</td>\n",
       "      <td>197</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.942202</td>\n",
       "      <td>6.632882</td>\n",
       "      <td>0.754167</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.606387</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.079268</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>0.122038</td>\n",
       "      <td>1508</td>\n",
       "      <td>38</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.034389</td>\n",
       "      <td>0.909320</td>\n",
       "      <td>3.587800</td>\n",
       "      <td>0.865009</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.664947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.724684</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.166020</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.184854</td>\n",
       "      <td>1702</td>\n",
       "      <td>176</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.775133</td>\n",
       "      <td>0.939499</td>\n",
       "      <td>9.948274</td>\n",
       "      <td>0.839160</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.661511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.968750</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.036548</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>0.325330</td>\n",
       "      <td>1719</td>\n",
       "      <td>112</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.539692</td>\n",
       "      <td>0.586751</td>\n",
       "      <td>10.652553</td>\n",
       "      <td>0.842814</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.658167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.710400</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.098882</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>1520</td>\n",
       "      <td>118</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.394882</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>1.140798</td>\n",
       "      <td>0.679537</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.884537</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.247053</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.926301</td>\n",
       "      <td>2017</td>\n",
       "      <td>71</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.914960</td>\n",
       "      <td>0.850039</td>\n",
       "      <td>5.494507</td>\n",
       "      <td>0.638164</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.748327</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.210652</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>0.637557</td>\n",
       "      <td>2570</td>\n",
       "      <td>223</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.382927</td>\n",
       "      <td>0.971712</td>\n",
       "      <td>9.489138</td>\n",
       "      <td>0.888692</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.694394</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.086820</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.710663</td>\n",
       "      <td>2881</td>\n",
       "      <td>72</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.439337</td>\n",
       "      <td>0.201719</td>\n",
       "      <td>9.957636</td>\n",
       "      <td>0.790148</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.825310</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.218655</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>0.604417</td>\n",
       "      <td>2678</td>\n",
       "      <td>201</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.203061</td>\n",
       "      <td>0.942854</td>\n",
       "      <td>6.988655</td>\n",
       "      <td>0.877914</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.952187</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.197306</td>\n",
       "      <td>8</td>\n",
       "      <td>66</td>\n",
       "      <td>0.105494</td>\n",
       "      <td>1795</td>\n",
       "      <td>58</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.218440</td>\n",
       "      <td>0.416510</td>\n",
       "      <td>9.832803</td>\n",
       "      <td>0.729738</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.661375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.648835</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.116889</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.227935</td>\n",
       "      <td>2981</td>\n",
       "      <td>151</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.818015</td>\n",
       "      <td>0.860731</td>\n",
       "      <td>1.069521</td>\n",
       "      <td>0.804299</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.660588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.766964</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.076632</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0.244126</td>\n",
       "      <td>2850</td>\n",
       "      <td>148</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.218764</td>\n",
       "      <td>0.558102</td>\n",
       "      <td>5.038362</td>\n",
       "      <td>0.625957</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.660745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.701566</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.084063</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.712271</td>\n",
       "      <td>2636</td>\n",
       "      <td>143</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>0.266781</td>\n",
       "      <td>10.766150</td>\n",
       "      <td>0.764415</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.613220</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.113521</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>0.680705</td>\n",
       "      <td>2076</td>\n",
       "      <td>202</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.552893</td>\n",
       "      <td>6.926967</td>\n",
       "      <td>0.632341</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.747862</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.082648</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>0.470301</td>\n",
       "      <td>2633</td>\n",
       "      <td>228</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.398824</td>\n",
       "      <td>0.816432</td>\n",
       "      <td>8.983451</td>\n",
       "      <td>0.660287</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.803280</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.218744</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>0.325959</td>\n",
       "      <td>2955</td>\n",
       "      <td>177</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.711150</td>\n",
       "      <td>0.809501</td>\n",
       "      <td>4.486660</td>\n",
       "      <td>0.638471</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.657196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.976209</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.129272</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>0.837710</td>\n",
       "      <td>2802</td>\n",
       "      <td>250</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.735216</td>\n",
       "      <td>0.209072</td>\n",
       "      <td>6.414480</td>\n",
       "      <td>0.878314</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.691420</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.062486</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>0.260829</td>\n",
       "      <td>1896</td>\n",
       "      <td>78</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.965419</td>\n",
       "      <td>0.558293</td>\n",
       "      <td>9.826363</td>\n",
       "      <td>0.675483</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.658008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.711549</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.220107</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>0.887086</td>\n",
       "      <td>2143</td>\n",
       "      <td>191</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.642032</td>\n",
       "      <td>0.084140</td>\n",
       "      <td>2.616287</td>\n",
       "      <td>0.959422</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.842572</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.012759</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>0.663502</td>\n",
       "      <td>2884</td>\n",
       "      <td>176</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.160808</td>\n",
       "      <td>0.548734</td>\n",
       "      <td>7.918952</td>\n",
       "      <td>0.860785</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.689708</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.223654</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>2939</td>\n",
       "      <td>158</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.744171</td>\n",
       "      <td>0.720940</td>\n",
       "      <td>4.080608</td>\n",
       "      <td>0.817016</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.656378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.803526</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>0.973011</td>\n",
       "      <td>2388</td>\n",
       "      <td>81</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.892047</td>\n",
       "      <td>0.631139</td>\n",
       "      <td>8.948113</td>\n",
       "      <td>0.801055</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.830762</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.157755</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>0.499193</td>\n",
       "      <td>2539</td>\n",
       "      <td>65</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.768554</td>\n",
       "      <td>0.043604</td>\n",
       "      <td>10.945505</td>\n",
       "      <td>0.787978</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.654149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.711824</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.275048</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>0.928319</td>\n",
       "      <td>1496</td>\n",
       "      <td>76</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.966655</td>\n",
       "      <td>0.963620</td>\n",
       "      <td>9.530095</td>\n",
       "      <td>0.717780</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.648785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.754039</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.265341</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0.556801</td>\n",
       "      <td>2758</td>\n",
       "      <td>84</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.696030</td>\n",
       "      <td>0.570061</td>\n",
       "      <td>1.971765</td>\n",
       "      <td>0.846003</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.996022</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.052025</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.322551</td>\n",
       "      <td>2127</td>\n",
       "      <td>191</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.136621</td>\n",
       "      <td>0.708911</td>\n",
       "      <td>6.528200</td>\n",
       "      <td>0.718604</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.658138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.767912</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.086862</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>0.511342</td>\n",
       "      <td>2848</td>\n",
       "      <td>66</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.798295</td>\n",
       "      <td>0.649964</td>\n",
       "      <td>8.019669</td>\n",
       "      <td>0.918317</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.654338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.956002</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.111399</td>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>0.438474</td>\n",
       "      <td>2982</td>\n",
       "      <td>116</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.328153</td>\n",
       "      <td>0.155042</td>\n",
       "      <td>10.818409</td>\n",
       "      <td>0.935573</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.944162</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.085075</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "      <td>0.303266</td>\n",
       "      <td>2002</td>\n",
       "      <td>218</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.326651</td>\n",
       "      <td>0.827869</td>\n",
       "      <td>3.715429</td>\n",
       "      <td>0.986101</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.658156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.782906</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.262607</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>0.699512</td>\n",
       "      <td>1935</td>\n",
       "      <td>82</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.132745</td>\n",
       "      <td>0.969537</td>\n",
       "      <td>8.145951</td>\n",
       "      <td>0.616427</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.759528</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.140056</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.250861</td>\n",
       "      <td>2497</td>\n",
       "      <td>147</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.080873</td>\n",
       "      <td>0.428314</td>\n",
       "      <td>7.884999</td>\n",
       "      <td>0.623277</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.659395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.966085</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.142706</td>\n",
       "      <td>8</td>\n",
       "      <td>57</td>\n",
       "      <td>0.182866</td>\n",
       "      <td>2260</td>\n",
       "      <td>147</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.638271</td>\n",
       "      <td>0.516696</td>\n",
       "      <td>7.571113</td>\n",
       "      <td>0.774269</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.661448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.892016</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.024315</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>0.158646</td>\n",
       "      <td>2109</td>\n",
       "      <td>215</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.341880</td>\n",
       "      <td>0.091799</td>\n",
       "      <td>1.941570</td>\n",
       "      <td>0.724565</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.662494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.991804</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.062599</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>0.491616</td>\n",
       "      <td>1550</td>\n",
       "      <td>130</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.173202</td>\n",
       "      <td>0.433852</td>\n",
       "      <td>4.985047</td>\n",
       "      <td>0.846340</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.854037</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.625860</td>\n",
       "      <td>1441</td>\n",
       "      <td>126</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.856490</td>\n",
       "      <td>0.658694</td>\n",
       "      <td>2.629344</td>\n",
       "      <td>0.628227</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.856968</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>5</td>\n",
       "      <td>93</td>\n",
       "      <td>0.575474</td>\n",
       "      <td>1689</td>\n",
       "      <td>193</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.643288</td>\n",
       "      <td>0.458253</td>\n",
       "      <td>6.456168</td>\n",
       "      <td>0.976586</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.754441</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.298357</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>0.195791</td>\n",
       "      <td>1396</td>\n",
       "      <td>42</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.100778</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>1.944430</td>\n",
       "      <td>0.873203</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.661257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.628475</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.105693</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>1417</td>\n",
       "      <td>145</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.281855</td>\n",
       "      <td>0.118165</td>\n",
       "      <td>7.967372</td>\n",
       "      <td>0.851577</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.667997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.950989</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.230521</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>0.177440</td>\n",
       "      <td>1456</td>\n",
       "      <td>182</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.806835</td>\n",
       "      <td>0.990505</td>\n",
       "      <td>5.126177</td>\n",
       "      <td>0.748807</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.660075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.910565</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.112241</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>0.238597</td>\n",
       "      <td>2718</td>\n",
       "      <td>43</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.739909</td>\n",
       "      <td>0.238236</td>\n",
       "      <td>4.777289</td>\n",
       "      <td>0.813731</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.659163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.798624</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.126885</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>0.099985</td>\n",
       "      <td>2635</td>\n",
       "      <td>170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.958541</td>\n",
       "      <td>0.847143</td>\n",
       "      <td>4.549052</td>\n",
       "      <td>0.982720</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.662903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.870708</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.154756</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>0.448446</td>\n",
       "      <td>2122</td>\n",
       "      <td>170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.328665</td>\n",
       "      <td>0.672518</td>\n",
       "      <td>8.523745</td>\n",
       "      <td>0.916632</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.915847</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.037362</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>0.057559</td>\n",
       "      <td>2246</td>\n",
       "      <td>152</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.441531</td>\n",
       "      <td>0.887704</td>\n",
       "      <td>4.509150</td>\n",
       "      <td>0.646827</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.657197</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.238453</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>0.101123</td>\n",
       "      <td>2542</td>\n",
       "      <td>204</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.700969</td>\n",
       "      <td>0.072763</td>\n",
       "      <td>9.218601</td>\n",
       "      <td>0.882497</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.662537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.632540</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.035451</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.374271</td>\n",
       "      <td>2856</td>\n",
       "      <td>116</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>0.947249</td>\n",
       "      <td>10.860011</td>\n",
       "      <td>0.901351</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.657213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.750504</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.035050</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>0.705084</td>\n",
       "      <td>1713</td>\n",
       "      <td>120</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.330253</td>\n",
       "      <td>0.434452</td>\n",
       "      <td>3.536822</td>\n",
       "      <td>0.762081</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.828589</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.232292</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>0.746045</td>\n",
       "      <td>2996</td>\n",
       "      <td>122</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.962173</td>\n",
       "      <td>0.374871</td>\n",
       "      <td>3.857121</td>\n",
       "      <td>0.947440</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.689438</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.298967</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0.969879</td>\n",
       "      <td>2852</td>\n",
       "      <td>77</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.891143</td>\n",
       "      <td>0.527701</td>\n",
       "      <td>10.929648</td>\n",
       "      <td>0.629519</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.648767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.821542</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.300791</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>0.629399</td>\n",
       "      <td>2549</td>\n",
       "      <td>173</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.454541</td>\n",
       "      <td>0.627558</td>\n",
       "      <td>6.843143</td>\n",
       "      <td>0.960463</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.618179</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.094289</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>0.106756</td>\n",
       "      <td>1372</td>\n",
       "      <td>38</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.169680</td>\n",
       "      <td>0.646848</td>\n",
       "      <td>4.882684</td>\n",
       "      <td>0.691758</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.706375</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.118104</td>\n",
       "      <td>6</td>\n",
       "      <td>73</td>\n",
       "      <td>0.453241</td>\n",
       "      <td>1958</td>\n",
       "      <td>175</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.279764</td>\n",
       "      <td>0.411207</td>\n",
       "      <td>7.027819</td>\n",
       "      <td>0.708383</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.653274</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>8</td>\n",
       "      <td>91</td>\n",
       "      <td>0.416639</td>\n",
       "      <td>1708</td>\n",
       "      <td>77</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.919177</td>\n",
       "      <td>0.082748</td>\n",
       "      <td>9.766615</td>\n",
       "      <td>0.820635</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.656296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.665934</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.133377</td>\n",
       "      <td>8</td>\n",
       "      <td>74</td>\n",
       "      <td>0.480370</td>\n",
       "      <td>2262</td>\n",
       "      <td>207</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.376739</td>\n",
       "      <td>0.749578</td>\n",
       "      <td>4.929894</td>\n",
       "      <td>0.931666</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.654717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.827633</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>1822</td>\n",
       "      <td>36</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.691714</td>\n",
       "      <td>0.534346</td>\n",
       "      <td>8.499107</td>\n",
       "      <td>0.965266</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.668943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.834060</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.227836</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>0.377851</td>\n",
       "      <td>2108</td>\n",
       "      <td>108</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.205045</td>\n",
       "      <td>0.251442</td>\n",
       "      <td>3.747318</td>\n",
       "      <td>0.682891</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.951288</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.237100</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>0.022185</td>\n",
       "      <td>1249</td>\n",
       "      <td>192</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.476211</td>\n",
       "      <td>0.831371</td>\n",
       "      <td>4.077772</td>\n",
       "      <td>0.926554</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.987190</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>5</td>\n",
       "      <td>66</td>\n",
       "      <td>0.589956</td>\n",
       "      <td>1766</td>\n",
       "      <td>77</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.420536</td>\n",
       "      <td>0.784668</td>\n",
       "      <td>7.393614</td>\n",
       "      <td>0.922018</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.961260</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.195179</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>0.608088</td>\n",
       "      <td>2873</td>\n",
       "      <td>60</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.554816</td>\n",
       "      <td>0.091002</td>\n",
       "      <td>8.263970</td>\n",
       "      <td>0.818979</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.780364</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.283141</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>0.697642</td>\n",
       "      <td>1796</td>\n",
       "      <td>162</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.459347</td>\n",
       "      <td>0.842091</td>\n",
       "      <td>8.689177</td>\n",
       "      <td>0.626494</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.618345</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.196242</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>0.209131</td>\n",
       "      <td>1042</td>\n",
       "      <td>233</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.075863</td>\n",
       "      <td>0.128880</td>\n",
       "      <td>2.280458</td>\n",
       "      <td>0.660761</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.660246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.655531</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.202262</td>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "      <td>0.411661</td>\n",
       "      <td>2887</td>\n",
       "      <td>225</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.515236</td>\n",
       "      <td>0.973110</td>\n",
       "      <td>7.019354</td>\n",
       "      <td>0.689540</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.928716</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.113525</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>1837</td>\n",
       "      <td>31</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.534424</td>\n",
       "      <td>0.355991</td>\n",
       "      <td>9.942173</td>\n",
       "      <td>0.651499</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.732040</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.106475</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>0.627900</td>\n",
       "      <td>1238</td>\n",
       "      <td>86</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.873579</td>\n",
       "      <td>0.920872</td>\n",
       "      <td>1.610780</td>\n",
       "      <td>0.710751</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.922481</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.234478</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>0.209349</td>\n",
       "      <td>1638</td>\n",
       "      <td>86</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.484523</td>\n",
       "      <td>0.618255</td>\n",
       "      <td>4.689136</td>\n",
       "      <td>0.785014</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.659151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.898988</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.021005</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.687166</td>\n",
       "      <td>1918</td>\n",
       "      <td>137</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.532113</td>\n",
       "      <td>0.107172</td>\n",
       "      <td>5.474124</td>\n",
       "      <td>0.813047</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.696988</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.090773</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>0.020071</td>\n",
       "      <td>2976</td>\n",
       "      <td>47</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.211448</td>\n",
       "      <td>0.327497</td>\n",
       "      <td>2.197621</td>\n",
       "      <td>0.956211</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.837437</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.213731</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0.086920</td>\n",
       "      <td>1110</td>\n",
       "      <td>244</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.586841</td>\n",
       "      <td>0.745439</td>\n",
       "      <td>5.316595</td>\n",
       "      <td>0.651032</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.664966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.713510</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.118925</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>0.570778</td>\n",
       "      <td>2975</td>\n",
       "      <td>89</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.986515</td>\n",
       "      <td>0.605775</td>\n",
       "      <td>3.372268</td>\n",
       "      <td>0.640713</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.661144</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.083787</td>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>0.087888</td>\n",
       "      <td>1349</td>\n",
       "      <td>125</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.055653</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>1.516355</td>\n",
       "      <td>0.607297</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.878785</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.309177</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0.575998</td>\n",
       "      <td>1074</td>\n",
       "      <td>34</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.975067</td>\n",
       "      <td>5.907488</td>\n",
       "      <td>0.889158</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.928345</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.225537</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.476619</td>\n",
       "      <td>1172</td>\n",
       "      <td>74</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.967994</td>\n",
       "      <td>8.109525</td>\n",
       "      <td>0.679803</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.894499</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.168952</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>0.767780</td>\n",
       "      <td>2573</td>\n",
       "      <td>67</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.506104</td>\n",
       "      <td>0.932014</td>\n",
       "      <td>4.206422</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.747692</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.146280</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>0.548922</td>\n",
       "      <td>1897</td>\n",
       "      <td>43</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.684572</td>\n",
       "      <td>0.087868</td>\n",
       "      <td>2.388248</td>\n",
       "      <td>0.601084</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.646678</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.151950</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>0.348868</td>\n",
       "      <td>2840</td>\n",
       "      <td>99</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.830619</td>\n",
       "      <td>0.965027</td>\n",
       "      <td>2.242972</td>\n",
       "      <td>0.892347</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.657433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.975336</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.064370</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.741121</td>\n",
       "      <td>1917</td>\n",
       "      <td>90</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.841829</td>\n",
       "      <td>0.139772</td>\n",
       "      <td>8.952673</td>\n",
       "      <td>0.680651</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.665462</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>0.665197</td>\n",
       "      <td>2433</td>\n",
       "      <td>92</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.358830</td>\n",
       "      <td>0.877201</td>\n",
       "      <td>4.924451</td>\n",
       "      <td>0.926640</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.775654</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.123083</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>0.539781</td>\n",
       "      <td>1574</td>\n",
       "      <td>173</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.577486</td>\n",
       "      <td>0.355362</td>\n",
       "      <td>4.914821</td>\n",
       "      <td>0.812743</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.654073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.626648</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.078708</td>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>0.431528</td>\n",
       "      <td>1578</td>\n",
       "      <td>165</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.730549</td>\n",
       "      <td>0.693718</td>\n",
       "      <td>2.667308</td>\n",
       "      <td>0.951452</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.798169</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.232438</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>0.997693</td>\n",
       "      <td>2464</td>\n",
       "      <td>127</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.706980</td>\n",
       "      <td>0.778572</td>\n",
       "      <td>2.431280</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.648488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.885626</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.158194</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>0.536481</td>\n",
       "      <td>1814</td>\n",
       "      <td>139</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.127489</td>\n",
       "      <td>0.397287</td>\n",
       "      <td>8.972954</td>\n",
       "      <td>0.659967</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.691701</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.226676</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0.641148</td>\n",
       "      <td>1754</td>\n",
       "      <td>194</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.542724</td>\n",
       "      <td>0.251799</td>\n",
       "      <td>4.456960</td>\n",
       "      <td>0.672639</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.963380</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.185018</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>0.287930</td>\n",
       "      <td>1362</td>\n",
       "      <td>170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.911852</td>\n",
       "      <td>0.139116</td>\n",
       "      <td>2.007946</td>\n",
       "      <td>0.702406</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.658261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.890438</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.187889</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>0.790085</td>\n",
       "      <td>2828</td>\n",
       "      <td>102</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.651367</td>\n",
       "      <td>0.771247</td>\n",
       "      <td>4.744354</td>\n",
       "      <td>0.627569</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.630928</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.041274</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>0.910686</td>\n",
       "      <td>2792</td>\n",
       "      <td>76</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.235906</td>\n",
       "      <td>0.165521</td>\n",
       "      <td>2.863209</td>\n",
       "      <td>0.934996</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.732859</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.103433</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>0.607894</td>\n",
       "      <td>2454</td>\n",
       "      <td>231</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.744250</td>\n",
       "      <td>0.205580</td>\n",
       "      <td>8.877899</td>\n",
       "      <td>0.841489</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.645707</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.134351</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>0.922960</td>\n",
       "      <td>2926</td>\n",
       "      <td>240</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.480837</td>\n",
       "      <td>0.918455</td>\n",
       "      <td>6.870682</td>\n",
       "      <td>0.613139</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.965100</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.084468</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0.165513</td>\n",
       "      <td>2524</td>\n",
       "      <td>118</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.311473</td>\n",
       "      <td>0.780496</td>\n",
       "      <td>3.775872</td>\n",
       "      <td>0.688039</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.662032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.685072</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.164552</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>0.458989</td>\n",
       "      <td>1147</td>\n",
       "      <td>148</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.860632</td>\n",
       "      <td>0.535070</td>\n",
       "      <td>2.844629</td>\n",
       "      <td>0.719838</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.723975</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.129195</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>0.898054</td>\n",
       "      <td>1031</td>\n",
       "      <td>188</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.036550</td>\n",
       "      <td>5.720669</td>\n",
       "      <td>0.825936</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.626283</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.242658</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>0.534456</td>\n",
       "      <td>2136</td>\n",
       "      <td>144</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.497012</td>\n",
       "      <td>0.765356</td>\n",
       "      <td>2.029815</td>\n",
       "      <td>0.733745</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.630201</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.235974</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>0.897430</td>\n",
       "      <td>1161</td>\n",
       "      <td>154</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.800750</td>\n",
       "      <td>0.978931</td>\n",
       "      <td>9.397889</td>\n",
       "      <td>0.946798</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.763194</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.175517</td>\n",
       "      <td>5</td>\n",
       "      <td>97</td>\n",
       "      <td>0.196113</td>\n",
       "      <td>2790</td>\n",
       "      <td>102</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.104109</td>\n",
       "      <td>0.799916</td>\n",
       "      <td>2.785447</td>\n",
       "      <td>0.861098</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.661597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.695273</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.039832</td>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "      <td>0.855696</td>\n",
       "      <td>1490</td>\n",
       "      <td>174</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.397184</td>\n",
       "      <td>0.668085</td>\n",
       "      <td>3.049843</td>\n",
       "      <td>0.717259</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.958534</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>0.207886</td>\n",
       "      <td>1146</td>\n",
       "      <td>142</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.181435</td>\n",
       "      <td>0.583042</td>\n",
       "      <td>5.214246</td>\n",
       "      <td>0.957069</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.661749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.926977</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.112545</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.363059</td>\n",
       "      <td>2142</td>\n",
       "      <td>170</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.830655</td>\n",
       "      <td>0.345192</td>\n",
       "      <td>8.738345</td>\n",
       "      <td>0.745103</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.656279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.944427</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.075853</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>0.779759</td>\n",
       "      <td>1592</td>\n",
       "      <td>232</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.565777</td>\n",
       "      <td>0.985367</td>\n",
       "      <td>5.710655</td>\n",
       "      <td>0.672843</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.793911</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.163737</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>0.698846</td>\n",
       "      <td>2304</td>\n",
       "      <td>133</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.218023</td>\n",
       "      <td>0.645054</td>\n",
       "      <td>5.217039</td>\n",
       "      <td>0.652910</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.960229</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.217367</td>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>0.822584</td>\n",
       "      <td>1127</td>\n",
       "      <td>115</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.815054</td>\n",
       "      <td>0.498091</td>\n",
       "      <td>1.671754</td>\n",
       "      <td>0.762099</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.798986</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.226061</td>\n",
       "      <td>7</td>\n",
       "      <td>95</td>\n",
       "      <td>0.136886</td>\n",
       "      <td>2848</td>\n",
       "      <td>61</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.873890</td>\n",
       "      <td>0.597413</td>\n",
       "      <td>7.005169</td>\n",
       "      <td>0.866015</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.661565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.670149</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.284324</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>0.383139</td>\n",
       "      <td>1633</td>\n",
       "      <td>76</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.046966</td>\n",
       "      <td>0.166283</td>\n",
       "      <td>8.380336</td>\n",
       "      <td>0.633119</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.841261</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.083605</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0.288694</td>\n",
       "      <td>2535</td>\n",
       "      <td>194</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.719046</td>\n",
       "      <td>0.297122</td>\n",
       "      <td>6.664046</td>\n",
       "      <td>0.790420</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.658948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.865468</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.291049</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>0.214940</td>\n",
       "      <td>2571</td>\n",
       "      <td>178</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.162014</td>\n",
       "      <td>0.841530</td>\n",
       "      <td>9.464078</td>\n",
       "      <td>0.982095</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.659390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.661869</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>0.075138</td>\n",
       "      <td>2040</td>\n",
       "      <td>159</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.495491</td>\n",
       "      <td>0.688402</td>\n",
       "      <td>5.348273</td>\n",
       "      <td>0.698561</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.664165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.927641</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.249825</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>0.272145</td>\n",
       "      <td>1865</td>\n",
       "      <td>42</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.360974</td>\n",
       "      <td>0.091582</td>\n",
       "      <td>10.173136</td>\n",
       "      <td>0.654727</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.659084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.980095</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.143802</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>0.541901</td>\n",
       "      <td>1215</td>\n",
       "      <td>113</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.732225</td>\n",
       "      <td>0.806561</td>\n",
       "      <td>7.587834</td>\n",
       "      <td>0.876911</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.939678</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0.221209</td>\n",
       "      <td>2945</td>\n",
       "      <td>139</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.669917</td>\n",
       "      <td>0.044165</td>\n",
       "      <td>6.882638</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.659002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.822323</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.151963</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>0.120523</td>\n",
       "      <td>2542</td>\n",
       "      <td>167</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.192599</td>\n",
       "      <td>0.115282</td>\n",
       "      <td>5.217627</td>\n",
       "      <td>0.918137</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.663365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.898026</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.026465</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0.522435</td>\n",
       "      <td>1614</td>\n",
       "      <td>47</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.650167</td>\n",
       "      <td>0.364697</td>\n",
       "      <td>6.606772</td>\n",
       "      <td>0.792425</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.953999</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.169080</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.404485</td>\n",
       "      <td>1038</td>\n",
       "      <td>45</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.803977</td>\n",
       "      <td>0.538401</td>\n",
       "      <td>7.616552</td>\n",
       "      <td>0.895142</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.808067</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.138045</td>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>2193</td>\n",
       "      <td>64</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.021269</td>\n",
       "      <td>0.874702</td>\n",
       "      <td>6.289371</td>\n",
       "      <td>0.975627</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.919513</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.309380</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>0.767188</td>\n",
       "      <td>1476</td>\n",
       "      <td>92</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.479876</td>\n",
       "      <td>0.627505</td>\n",
       "      <td>9.736771</td>\n",
       "      <td>0.993633</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.907309</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.135330</td>\n",
       "      <td>9</td>\n",
       "      <td>93</td>\n",
       "      <td>0.737582</td>\n",
       "      <td>2712</td>\n",
       "      <td>33</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.467311</td>\n",
       "      <td>0.297003</td>\n",
       "      <td>9.528218</td>\n",
       "      <td>0.886221</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.835643</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.093180</td>\n",
       "      <td>7</td>\n",
       "      <td>88</td>\n",
       "      <td>0.427773</td>\n",
       "      <td>1816</td>\n",
       "      <td>151</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.679647</td>\n",
       "      <td>0.218254</td>\n",
       "      <td>10.499612</td>\n",
       "      <td>0.914538</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.635764</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.135274</td>\n",
       "      <td>9</td>\n",
       "      <td>55</td>\n",
       "      <td>0.600798</td>\n",
       "      <td>2085</td>\n",
       "      <td>107</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.989185</td>\n",
       "      <td>0.743353</td>\n",
       "      <td>1.649864</td>\n",
       "      <td>0.760632</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.935130</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.079171</td>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>0.046945</td>\n",
       "      <td>1824</td>\n",
       "      <td>248</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.036732</td>\n",
       "      <td>0.957270</td>\n",
       "      <td>9.262057</td>\n",
       "      <td>0.920308</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.851911</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.074897</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0.994908</td>\n",
       "      <td>2596</td>\n",
       "      <td>153</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.396242</td>\n",
       "      <td>0.758238</td>\n",
       "      <td>7.960206</td>\n",
       "      <td>0.661558</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.647042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.926333</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.077332</td>\n",
       "      <td>7</td>\n",
       "      <td>98</td>\n",
       "      <td>0.592940</td>\n",
       "      <td>2933</td>\n",
       "      <td>131</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.091487</td>\n",
       "      <td>0.877461</td>\n",
       "      <td>3.656000</td>\n",
       "      <td>0.651806</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.955499</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.296695</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>0.809516</td>\n",
       "      <td>1039</td>\n",
       "      <td>55</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.550857</td>\n",
       "      <td>0.086987</td>\n",
       "      <td>5.084532</td>\n",
       "      <td>0.749075</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.703902</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.227026</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>0.081046</td>\n",
       "      <td>2339</td>\n",
       "      <td>206</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.683259</td>\n",
       "      <td>0.076131</td>\n",
       "      <td>9.512069</td>\n",
       "      <td>0.798059</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.792235</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.187722</td>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>0.347809</td>\n",
       "      <td>2889</td>\n",
       "      <td>70</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.565732</td>\n",
       "      <td>0.267028</td>\n",
       "      <td>9.786300</td>\n",
       "      <td>0.918970</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.657066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.863381</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.265175</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>0.708363</td>\n",
       "      <td>1362</td>\n",
       "      <td>75</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.697471</td>\n",
       "      <td>0.680141</td>\n",
       "      <td>7.186114</td>\n",
       "      <td>0.901087</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.663442</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.274261</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.825817</td>\n",
       "      <td>2417</td>\n",
       "      <td>190</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.335119</td>\n",
       "      <td>0.743508</td>\n",
       "      <td>2.607599</td>\n",
       "      <td>0.927187</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.932854</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.162240</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>0.287038</td>\n",
       "      <td>2069</td>\n",
       "      <td>185</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.981186</td>\n",
       "      <td>0.631814</td>\n",
       "      <td>3.598036</td>\n",
       "      <td>0.853602</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.657647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.815994</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.243954</td>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>0.761028</td>\n",
       "      <td>1585</td>\n",
       "      <td>254</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.840122</td>\n",
       "      <td>0.392151</td>\n",
       "      <td>2.794973</td>\n",
       "      <td>0.901480</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.915523</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>0.300964</td>\n",
       "      <td>2040</td>\n",
       "      <td>42</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.067351</td>\n",
       "      <td>0.582170</td>\n",
       "      <td>4.458831</td>\n",
       "      <td>0.848366</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.658653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.618297</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.271461</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>0.445958</td>\n",
       "      <td>1762</td>\n",
       "      <td>210</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.758263</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>1.221236</td>\n",
       "      <td>0.729444</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.795457</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.241122</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>0.445903</td>\n",
       "      <td>2453</td>\n",
       "      <td>59</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.944304</td>\n",
       "      <td>0.102194</td>\n",
       "      <td>9.542098</td>\n",
       "      <td>0.810810</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.654699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.930368</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.099512</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>0.680499</td>\n",
       "      <td>2045</td>\n",
       "      <td>233</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.273260</td>\n",
       "      <td>0.950864</td>\n",
       "      <td>2.510579</td>\n",
       "      <td>0.772934</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.977446</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.135918</td>\n",
       "      <td>5</td>\n",
       "      <td>93</td>\n",
       "      <td>0.397594</td>\n",
       "      <td>2976</td>\n",
       "      <td>227</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.983978</td>\n",
       "      <td>0.409334</td>\n",
       "      <td>9.940992</td>\n",
       "      <td>0.691982</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.685242</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.019340</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>0.368526</td>\n",
       "      <td>2294</td>\n",
       "      <td>238</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.543233</td>\n",
       "      <td>0.159246</td>\n",
       "      <td>3.037012</td>\n",
       "      <td>0.636466</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.661463</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.146108</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>0.626220</td>\n",
       "      <td>1959</td>\n",
       "      <td>202</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.920848</td>\n",
       "      <td>7.166503</td>\n",
       "      <td>0.918615</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.792609</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.045192</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>0.430306</td>\n",
       "      <td>2701</td>\n",
       "      <td>206</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.491595</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>6.819714</td>\n",
       "      <td>0.707597</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.919024</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.103109</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>1273</td>\n",
       "      <td>252</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.392494</td>\n",
       "      <td>0.479939</td>\n",
       "      <td>7.000205</td>\n",
       "      <td>0.716665</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.663678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.877993</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.268037</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>0.039619</td>\n",
       "      <td>1213</td>\n",
       "      <td>209</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.104930</td>\n",
       "      <td>0.242045</td>\n",
       "      <td>10.866626</td>\n",
       "      <td>0.656998</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.656548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.799555</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.195447</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>0.559649</td>\n",
       "      <td>2951</td>\n",
       "      <td>127</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.517712</td>\n",
       "      <td>0.087866</td>\n",
       "      <td>4.506269</td>\n",
       "      <td>0.613281</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.631431</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.129077</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>0.567541</td>\n",
       "      <td>2709</td>\n",
       "      <td>152</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.200150</td>\n",
       "      <td>2.674826</td>\n",
       "      <td>0.641827</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.854572</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.221943</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.936212</td>\n",
       "      <td>2382</td>\n",
       "      <td>253</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.541296</td>\n",
       "      <td>0.709061</td>\n",
       "      <td>9.709691</td>\n",
       "      <td>0.885635</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.648729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.920691</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.111835</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.660820</td>\n",
       "      <td>1207</td>\n",
       "      <td>162</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.597476</td>\n",
       "      <td>0.783833</td>\n",
       "      <td>5.809038</td>\n",
       "      <td>0.616302</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.651567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.665600</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.141582</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.247103</td>\n",
       "      <td>1908</td>\n",
       "      <td>55</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.871784</td>\n",
       "      <td>0.219214</td>\n",
       "      <td>10.758653</td>\n",
       "      <td>0.734758</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.659035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.672847</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.246910</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>0.498196</td>\n",
       "      <td>2617</td>\n",
       "      <td>228</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.719202</td>\n",
       "      <td>0.228455</td>\n",
       "      <td>10.963339</td>\n",
       "      <td>0.989917</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.653505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.860130</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0.072198</td>\n",
       "      <td>2982</td>\n",
       "      <td>237</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.257683</td>\n",
       "      <td>0.462623</td>\n",
       "      <td>9.682725</td>\n",
       "      <td>0.890868</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.665478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.897083</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.137648</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0.504845</td>\n",
       "      <td>2585</td>\n",
       "      <td>207</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.513996</td>\n",
       "      <td>0.588058</td>\n",
       "      <td>1.293944</td>\n",
       "      <td>0.758206</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.842475</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.189317</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.655009</td>\n",
       "      <td>1754</td>\n",
       "      <td>145</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.283855</td>\n",
       "      <td>0.359151</td>\n",
       "      <td>4.838472</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.652634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.934399</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.080738</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.405310</td>\n",
       "      <td>1477</td>\n",
       "      <td>117</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.632541</td>\n",
       "      <td>0.372849</td>\n",
       "      <td>6.126152</td>\n",
       "      <td>0.765212</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.627653</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.141150</td>\n",
       "      <td>8</td>\n",
       "      <td>69</td>\n",
       "      <td>0.405021</td>\n",
       "      <td>1013</td>\n",
       "      <td>247</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.662621</td>\n",
       "      <td>0.095262</td>\n",
       "      <td>7.521232</td>\n",
       "      <td>0.724859</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.655687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.759618</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.267396</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>0.773473</td>\n",
       "      <td>2975</td>\n",
       "      <td>194</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.453790</td>\n",
       "      <td>3.360505</td>\n",
       "      <td>0.629399</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.650601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.667903</td>\n",
       "      <td>gpu</td>\n",
       "      <td>0.165932</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>0.828883</td>\n",
       "      <td>2280</td>\n",
       "      <td>163</td>\n",
       "      <td>regression</td>\n",
       "      <td>0.248714</td>\n",
       "      <td>0.617145</td>\n",
       "      <td>8.067772</td>\n",
       "      <td>0.666817</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.649380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     colsample_bytree device  learning_rate  max_depth  min_child_samples  \\\n",
       "0            0.749816    gpu       0.295214          5                 81   \n",
       "1            0.883229    gpu       0.016175          4                 97   \n",
       "2            0.772778    gpu       0.097369          5                 51   \n",
       "3            0.993292    gpu       0.150029          7                 60   \n",
       "4            0.606387    gpu       0.079268          6                 69   \n",
       "5            0.724684    gpu       0.166020          4                 13   \n",
       "6            0.968750    gpu       0.036548          9                 71   \n",
       "7            0.710400    gpu       0.098882          7                 74   \n",
       "8            0.884537    gpu       0.247053          5                 14   \n",
       "9            0.748327    gpu       0.210652          7                 53   \n",
       "10           0.694394    gpu       0.086820          5                 18   \n",
       "11           0.825310    gpu       0.218655          7                 32   \n",
       "12           0.952187    gpu       0.197306          8                 66   \n",
       "13           0.648835    gpu       0.116889          8                 10   \n",
       "14           0.766964    gpu       0.076632          5                 53   \n",
       "15           0.701566    gpu       0.084063          9                 11   \n",
       "16           0.613220    gpu       0.113521          3                 28   \n",
       "17           0.747862    gpu       0.082648          8                 33   \n",
       "18           0.803280    gpu       0.218744          5                 29   \n",
       "19           0.976209    gpu       0.129272          9                 91   \n",
       "20           0.691420    gpu       0.062486          3                 69   \n",
       "21           0.711549    gpu       0.220107          5                 29   \n",
       "22           0.842572    gpu       0.012759          8                 98   \n",
       "23           0.689708    gpu       0.223654          3                 23   \n",
       "24           0.803526    gpu       0.200900          8                 39   \n",
       "25           0.830762    gpu       0.157755          7                 70   \n",
       "26           0.711824    gpu       0.275048          6                 46   \n",
       "27           0.754039    gpu       0.265341          5                 27   \n",
       "28           0.996022    gpu       0.052025          5                 11   \n",
       "29           0.767912    gpu       0.086862          7                 69   \n",
       "30           0.956002    gpu       0.111399          6                 75   \n",
       "31           0.944162    gpu       0.085075          4                 79   \n",
       "32           0.782906    gpu       0.262607          6                 56   \n",
       "33           0.759528    gpu       0.140056          4                 64   \n",
       "34           0.966085    gpu       0.142706          8                 57   \n",
       "35           0.892016    gpu       0.024315          4                 25   \n",
       "36           0.991804    gpu       0.062599          4                 99   \n",
       "37           0.854037    gpu       0.023591          9                 19   \n",
       "38           0.856968    gpu       0.017953          5                 93   \n",
       "39           0.754441    gpu       0.298357          4                 52   \n",
       "40           0.628475    gpu       0.105693          4                 83   \n",
       "41           0.950989    gpu       0.230521          7                 45   \n",
       "42           0.910565    gpu       0.112241          8                 46   \n",
       "43           0.798624    gpu       0.126885          7                 68   \n",
       "44           0.870708    gpu       0.154756          4                 25   \n",
       "45           0.915847    gpu       0.037362          4                 67   \n",
       "46           0.657197    gpu       0.238453          6                 26   \n",
       "47           0.632540    gpu       0.035451          5                 17   \n",
       "48           0.750504    gpu       0.035050          7                 63   \n",
       "49           0.828589    gpu       0.232292          7                 48   \n",
       "50           0.689438    gpu       0.298967          6                 54   \n",
       "51           0.821542    gpu       0.300791          4                 56   \n",
       "52           0.618179    gpu       0.094289          8                 85   \n",
       "53           0.706375    gpu       0.118104          6                 73   \n",
       "54           0.653274    gpu       0.032860          8                 91   \n",
       "55           0.665934    gpu       0.133377          8                 74   \n",
       "56           0.827633    gpu       0.029054          3                 10   \n",
       "57           0.834060    gpu       0.227836          4                 83   \n",
       "58           0.951288    gpu       0.237100          4                 81   \n",
       "59           0.987190    gpu       0.036522          5                 66   \n",
       "60           0.961260    gpu       0.195179          4                 47   \n",
       "61           0.780364    gpu       0.283141          8                 60   \n",
       "62           0.618345    gpu       0.196242          4                 86   \n",
       "63           0.655531    gpu       0.202262          6                 65   \n",
       "64           0.928716    gpu       0.113525          4                 70   \n",
       "65           0.732040    gpu       0.106475          4                 56   \n",
       "66           0.922481    gpu       0.234478          7                 61   \n",
       "67           0.898988    gpu       0.021005          5                 16   \n",
       "68           0.696988    gpu       0.090773          7                 37   \n",
       "69           0.837437    gpu       0.213731          5                 27   \n",
       "70           0.713510    gpu       0.118925          4                 73   \n",
       "71           0.661144    gpu       0.083787          6                 84   \n",
       "72           0.878785    gpu       0.309177          8                 75   \n",
       "73           0.928345    gpu       0.225537          5                 17   \n",
       "74           0.894499    gpu       0.168952          3                 99   \n",
       "75           0.747692    gpu       0.146280          7                 32   \n",
       "76           0.646678    gpu       0.151950          8                 44   \n",
       "77           0.975336    gpu       0.064370          5                 10   \n",
       "78           0.665462    gpu       0.059280          7                 92   \n",
       "79           0.775654    gpu       0.123083          5                 47   \n",
       "80           0.626648    gpu       0.078708          9                 60   \n",
       "81           0.798169    gpu       0.232438          5                 45   \n",
       "82           0.885626    gpu       0.158194          4                 47   \n",
       "83           0.691701    gpu       0.226676          4                 36   \n",
       "84           0.963380    gpu       0.185018          9                 56   \n",
       "85           0.890438    gpu       0.187889          6                 77   \n",
       "86           0.630928    gpu       0.041274          3                 95   \n",
       "87           0.732859    gpu       0.103433          5                 49   \n",
       "88           0.645707    gpu       0.134351          4                 84   \n",
       "89           0.965100    gpu       0.084468          3                 80   \n",
       "90           0.685072    gpu       0.164552          3                 93   \n",
       "91           0.723975    gpu       0.129195          5                 65   \n",
       "92           0.626283    gpu       0.242658          9                 75   \n",
       "93           0.630201    gpu       0.235974          4                 83   \n",
       "94           0.763194    gpu       0.175517          5                 97   \n",
       "95           0.695273    gpu       0.039832          8                 41   \n",
       "96           0.958534    gpu       0.013901          9                 42   \n",
       "97           0.926977    gpu       0.112545          4                 11   \n",
       "98           0.944427    gpu       0.075853          4                 95   \n",
       "99           0.793911    gpu       0.163737          3                 49   \n",
       "100          0.960229    gpu       0.217367          7                 84   \n",
       "101          0.798986    gpu       0.226061          7                 95   \n",
       "102          0.670149    gpu       0.284324          4                 94   \n",
       "103          0.841261    gpu       0.083605          7                 20   \n",
       "104          0.865468    gpu       0.291049          7                 65   \n",
       "105          0.661869    gpu       0.197000          4                 82   \n",
       "106          0.927641    gpu       0.249825          6                 60   \n",
       "107          0.980095    gpu       0.143802          9                 21   \n",
       "108          0.939678    gpu       0.084900          3                 21   \n",
       "109          0.822323    gpu       0.151963          9                 23   \n",
       "110          0.898026    gpu       0.026465          5                 27   \n",
       "111          0.953999    gpu       0.169080          4                 64   \n",
       "112          0.808067    gpu       0.138045          6                 75   \n",
       "113          0.919513    gpu       0.309380          6                 38   \n",
       "114          0.907309    gpu       0.135330          9                 93   \n",
       "115          0.835643    gpu       0.093180          7                 88   \n",
       "116          0.635764    gpu       0.135274          9                 55   \n",
       "117          0.935130    gpu       0.079171          5                 83   \n",
       "118          0.851911    gpu       0.074897          3                 29   \n",
       "119          0.926333    gpu       0.077332          7                 98   \n",
       "120          0.955499    gpu       0.296695          6                 51   \n",
       "121          0.703902    gpu       0.227026          4                 27   \n",
       "122          0.792235    gpu       0.187722          4                 76   \n",
       "123          0.863381    gpu       0.265175          5                 57   \n",
       "124          0.663442    gpu       0.274261          4                 40   \n",
       "125          0.932854    gpu       0.162240          9                 75   \n",
       "126          0.815994    gpu       0.243954          8                 92   \n",
       "127          0.915523    gpu       0.062126          6                 77   \n",
       "128          0.618297    gpu       0.271461          9                 38   \n",
       "129          0.795457    gpu       0.241122          4                 96   \n",
       "130          0.930368    gpu       0.099512          4                 56   \n",
       "131          0.977446    gpu       0.135918          5                 93   \n",
       "132          0.685242    gpu       0.019340          3                 33   \n",
       "133          0.661463    gpu       0.146108          4                 80   \n",
       "134          0.792609    gpu       0.045192          3                 93   \n",
       "135          0.919024    gpu       0.103109          7                 90   \n",
       "136          0.877993    gpu       0.268037          8                 50   \n",
       "137          0.799555    gpu       0.195447          6                 42   \n",
       "138          0.631431    gpu       0.129077          4                 47   \n",
       "139          0.854572    gpu       0.221943          8                 18   \n",
       "140          0.920691    gpu       0.111835          5                 16   \n",
       "141          0.665600    gpu       0.141582          4                 21   \n",
       "142          0.672847    gpu       0.246910          9                 41   \n",
       "143          0.860130    gpu       0.069863          5                 70   \n",
       "144          0.897083    gpu       0.137648          6                 29   \n",
       "145          0.842475    gpu       0.189317          6                 15   \n",
       "146          0.934399    gpu       0.080738          4                 17   \n",
       "147          0.627653    gpu       0.141150          8                 69   \n",
       "148          0.759618    gpu       0.267396          7                 74   \n",
       "149          0.667903    gpu       0.165932          4                 85   \n",
       "\n",
       "     min_split_gain  n_estimators  num_leaves   objective  reg_alpha  \\\n",
       "0          0.598658          2638         152  regression   0.155995   \n",
       "1          0.832443          1805         160  regression   0.181825   \n",
       "2          0.046666          1699         238  regression   0.232771   \n",
       "3          0.680308          1840         197  regression   0.013265   \n",
       "4          0.122038          1508          38  regression   0.034389   \n",
       "5          0.184854          1702         176  regression   0.775133   \n",
       "6          0.325330          1719         112  regression   0.539692   \n",
       "7          0.015636          1520         118  regression   0.394882   \n",
       "8          0.926301          2017          71  regression   0.914960   \n",
       "9          0.637557          2570         223  regression   0.382927   \n",
       "10         0.710663          2881          72  regression   0.439337   \n",
       "11         0.604417          2678         201  regression   0.203061   \n",
       "12         0.105494          1795          58  regression   0.218440   \n",
       "13         0.227935          2981         151  regression   0.818015   \n",
       "14         0.244126          2850         148  regression   0.218764   \n",
       "15         0.712271          2636         143  regression   0.997740   \n",
       "16         0.680705          2076         202  regression   0.447783   \n",
       "17         0.470301          2633         228  regression   0.398824   \n",
       "18         0.325959          2955         177  regression   0.711150   \n",
       "19         0.837710          2802         250  regression   0.735216   \n",
       "20         0.260829          1896          78  regression   0.965419   \n",
       "21         0.887086          2143         191  regression   0.642032   \n",
       "22         0.663502          2884         176  regression   0.160808   \n",
       "23         0.325400          2939         158  regression   0.744171   \n",
       "24         0.973011          2388          81  regression   0.892047   \n",
       "25         0.499193          2539          65  regression   0.768554   \n",
       "26         0.928319          1496          76  regression   0.966655   \n",
       "27         0.556801          2758          84  regression   0.696030   \n",
       "28         0.322551          2127         191  regression   0.136621   \n",
       "29         0.511342          2848          66  regression   0.798295   \n",
       "30         0.438474          2982         116  regression   0.328153   \n",
       "31         0.303266          2002         218  regression   0.326651   \n",
       "32         0.699512          1935          82  regression   0.132745   \n",
       "33         0.250861          2497         147  regression   0.080873   \n",
       "34         0.182866          2260         147  regression   0.638271   \n",
       "35         0.158646          2109         215  regression   0.341880   \n",
       "36         0.491616          1550         130  regression   0.173202   \n",
       "37         0.625860          1441         126  regression   0.856490   \n",
       "38         0.575474          1689         193  regression   0.643288   \n",
       "39         0.195791          1396          42  regression   0.100778   \n",
       "40         0.023272          1417         145  regression   0.281855   \n",
       "41         0.177440          1456         182  regression   0.806835   \n",
       "42         0.238597          2718          43  regression   0.739909   \n",
       "43         0.099985          2635         170  regression   0.958541   \n",
       "44         0.448446          2122         170  regression   0.328665   \n",
       "45         0.057559          2246         152  regression   0.441531   \n",
       "46         0.101123          2542         204  regression   0.700969   \n",
       "47         0.374271          2856         116  regression   0.812800   \n",
       "48         0.705084          1713         120  regression   0.330253   \n",
       "49         0.746045          2996         122  regression   0.962173   \n",
       "50         0.969879          2852          77  regression   0.891143   \n",
       "51         0.629399          2549         173  regression   0.454541   \n",
       "52         0.106756          1372          38  regression   0.169680   \n",
       "53         0.453241          1958         175  regression   0.279764   \n",
       "54         0.416639          1708          77  regression   0.919177   \n",
       "55         0.480370          2262         207  regression   0.376739   \n",
       "56         0.013672          1822          36  regression   0.691714   \n",
       "57         0.377851          2108         108  regression   0.205045   \n",
       "58         0.022185          1249         192  regression   0.476211   \n",
       "59         0.589956          1766          77  regression   0.420536   \n",
       "60         0.608088          2873          60  regression   0.554816   \n",
       "61         0.697642          1796         162  regression   0.459347   \n",
       "62         0.209131          1042         233  regression   0.075863   \n",
       "63         0.411661          2887         225  regression   0.515236   \n",
       "64         0.031805          1837          31  regression   0.534424   \n",
       "65         0.627900          1238          86  regression   0.873579   \n",
       "66         0.209349          1638          86  regression   0.484523   \n",
       "67         0.687166          1918         137  regression   0.532113   \n",
       "68         0.020071          2976          47  regression   0.211448   \n",
       "69         0.086920          1110         244  regression   0.586841   \n",
       "70         0.570778          2975          89  regression   0.986515   \n",
       "71         0.087888          1349         125  regression   0.055653   \n",
       "72         0.575998          1074          34  regression   0.005300   \n",
       "73         0.476619          1172          74  regression   0.205078   \n",
       "74         0.767780          2573          67  regression   0.506104   \n",
       "75         0.548922          1897          43  regression   0.684572   \n",
       "76         0.348868          2840          99  regression   0.830619   \n",
       "77         0.741121          1917          90  regression   0.841829   \n",
       "78         0.665197          2433          92  regression   0.358830   \n",
       "79         0.539781          1574         173  regression   0.577486   \n",
       "80         0.431528          1578         165  regression   0.730549   \n",
       "81         0.997693          2464         127  regression   0.706980   \n",
       "82         0.536481          1814         139  regression   0.127489   \n",
       "83         0.641148          1754         194  regression   0.542724   \n",
       "84         0.287930          1362         170  regression   0.911852   \n",
       "85         0.790085          2828         102  regression   0.651367   \n",
       "86         0.910686          2792          76  regression   0.235906   \n",
       "87         0.607894          2454         231  regression   0.744250   \n",
       "88         0.922960          2926         240  regression   0.480837   \n",
       "89         0.165513          2524         118  regression   0.311473   \n",
       "90         0.458989          1147         148  regression   0.860632   \n",
       "91         0.898054          1031         188  regression   0.190688   \n",
       "92         0.534456          2136         144  regression   0.497012   \n",
       "93         0.897430          1161         154  regression   0.800750   \n",
       "94         0.196113          2790         102  regression   0.104109   \n",
       "95         0.855696          1490         174  regression   0.397184   \n",
       "96         0.207886          1146         142  regression   0.181435   \n",
       "97         0.363059          2142         170  regression   0.830655   \n",
       "98         0.779759          1592         232  regression   0.565777   \n",
       "99         0.698846          2304         133  regression   0.218023   \n",
       "100        0.822584          1127         115  regression   0.815054   \n",
       "101        0.136886          2848          61  regression   0.873890   \n",
       "102        0.383139          1633          76  regression   0.046966   \n",
       "103        0.288694          2535         194  regression   0.719046   \n",
       "104        0.214940          2571         178  regression   0.162014   \n",
       "105        0.075138          2040         159  regression   0.495491   \n",
       "106        0.272145          1865          42  regression   0.360974   \n",
       "107        0.541901          1215         113  regression   0.732225   \n",
       "108        0.221209          2945         139  regression   0.669917   \n",
       "109        0.120523          2542         167  regression   0.192599   \n",
       "110        0.522435          1614          47  regression   0.650167   \n",
       "111        0.404485          1038          45  regression   0.803977   \n",
       "112        0.024401          2193          64  regression   0.021269   \n",
       "113        0.767188          1476          92  regression   0.479876   \n",
       "114        0.737582          2712          33  regression   0.467311   \n",
       "115        0.427773          1816         151  regression   0.679647   \n",
       "116        0.600798          2085         107  regression   0.989185   \n",
       "117        0.046945          1824         248  regression   0.036732   \n",
       "118        0.994908          2596         153  regression   0.396242   \n",
       "119        0.592940          2933         131  regression   0.091487   \n",
       "120        0.809516          1039          55  regression   0.550857   \n",
       "121        0.081046          2339         206  regression   0.683259   \n",
       "122        0.347809          2889          70  regression   0.565732   \n",
       "123        0.708363          1362          75  regression   0.697471   \n",
       "124        0.825817          2417         190  regression   0.335119   \n",
       "125        0.287038          2069         185  regression   0.981186   \n",
       "126        0.761028          1585         254  regression   0.840122   \n",
       "127        0.300964          2040          42  regression   0.067351   \n",
       "128        0.445958          1762         210  regression   0.758263   \n",
       "129        0.445903          2453          59  regression   0.944304   \n",
       "130        0.680499          2045         233  regression   0.273260   \n",
       "131        0.397594          2976         227  regression   0.983978   \n",
       "132        0.368526          2294         238  regression   0.543233   \n",
       "133        0.626220          1959         202  regression   0.032526   \n",
       "134        0.430306          2701         206  regression   0.491595   \n",
       "135        0.011621          1273         252  regression   0.392494   \n",
       "136        0.039619          1213         209  regression   0.104930   \n",
       "137        0.559649          2951         127  regression   0.517712   \n",
       "138        0.567541          2709         152  regression   0.800587   \n",
       "139        0.936212          2382         253  regression   0.541296   \n",
       "140        0.660820          1207         162  regression   0.597476   \n",
       "141        0.247103          1908          55  regression   0.871784   \n",
       "142        0.498196          2617         228  regression   0.719202   \n",
       "143        0.072198          2982         237  regression   0.257683   \n",
       "144        0.504845          2585         207  regression   0.513996   \n",
       "145        0.655009          1754         145  regression   0.283855   \n",
       "146        0.405310          1477         117  regression   0.632541   \n",
       "147        0.405021          1013         247  regression   0.662621   \n",
       "148        0.773473          2975         194  regression   0.969821   \n",
       "149        0.828883          2280         163  regression   0.248714   \n",
       "\n",
       "     reg_lambda  scale_pos_weight  subsample  verbose     score  \n",
       "0      0.058084          9.661761   0.840446       -1  0.652585  \n",
       "1      0.183405          4.042422   0.809903       -1  0.649241  \n",
       "2      0.090606          7.183860   0.752985       -1  0.666127  \n",
       "3      0.942202          6.632882   0.754167       -1  0.650983  \n",
       "4      0.909320          3.587800   0.865009       -1  0.664947  \n",
       "5      0.939499          9.948274   0.839160       -1  0.661511  \n",
       "6      0.586751         10.652553   0.842814       -1  0.658167  \n",
       "7      0.293488          1.140798   0.679537       -1  0.665802  \n",
       "8      0.850039          5.494507   0.638164       -1  0.650325  \n",
       "9      0.971712          9.489138   0.888692       -1  0.651702  \n",
       "10     0.201719          9.957636   0.790148       -1  0.652179  \n",
       "11     0.942854          6.988655   0.877914       -1  0.653023  \n",
       "12     0.416510          9.832803   0.729738       -1  0.661375  \n",
       "13     0.860731          1.069521   0.804299       -1  0.660588  \n",
       "14     0.558102          5.038362   0.625957       -1  0.660745  \n",
       "15     0.266781         10.766150   0.764415       -1  0.651837  \n",
       "16     0.552893          6.926967   0.632341       -1  0.649942  \n",
       "17     0.816432          8.983451   0.660287       -1  0.655220  \n",
       "18     0.809501          4.486660   0.638471       -1  0.657196  \n",
       "19     0.209072          6.414480   0.878314       -1  0.649573  \n",
       "20     0.558293          9.826363   0.675483       -1  0.658008  \n",
       "21     0.084140          2.616287   0.959422       -1  0.650078  \n",
       "22     0.548734          7.918952   0.860785       -1  0.652180  \n",
       "23     0.720940          4.080608   0.817016       -1  0.656378  \n",
       "24     0.631139          8.948113   0.801055       -1  0.649878  \n",
       "25     0.043604         10.945505   0.787978       -1  0.654149  \n",
       "26     0.963620          9.530095   0.717780       -1  0.648785  \n",
       "27     0.570061          1.971765   0.846003       -1  0.653453  \n",
       "28     0.708911          6.528200   0.718604       -1  0.658138  \n",
       "29     0.649964          8.019669   0.918317       -1  0.654338  \n",
       "30     0.155042         10.818409   0.935573       -1  0.655253  \n",
       "31     0.827869          3.715429   0.986101       -1  0.658156  \n",
       "32     0.969537          8.145951   0.616427       -1  0.651442  \n",
       "33     0.428314          7.884999   0.623277       -1  0.659395  \n",
       "34     0.516696          7.571113   0.774269       -1  0.661448  \n",
       "35     0.091799          1.941570   0.724565       -1  0.662494  \n",
       "36     0.433852          4.985047   0.846340       -1  0.653308  \n",
       "37     0.658694          2.629344   0.628227       -1  0.652176  \n",
       "38     0.458253          6.456168   0.976586       -1  0.652590  \n",
       "39     0.018222          1.944430   0.873203       -1  0.661257  \n",
       "40     0.118165          7.967372   0.851577       -1  0.667997  \n",
       "41     0.990505          5.126177   0.748807       -1  0.660075  \n",
       "42     0.238236          4.777289   0.813731       -1  0.659163  \n",
       "43     0.847143          4.549052   0.982720       -1  0.662903  \n",
       "44     0.672518          8.523745   0.916632       -1  0.653932  \n",
       "45     0.887704          4.509150   0.646827       -1  0.666928  \n",
       "46     0.072763          9.218601   0.882497       -1  0.662537  \n",
       "47     0.947249         10.860011   0.901351       -1  0.657213  \n",
       "48     0.434452          3.536822   0.762081       -1  0.651836  \n",
       "49     0.374871          3.857121   0.947440       -1  0.650660  \n",
       "50     0.527701         10.929648   0.629519       -1  0.648767  \n",
       "51     0.627558          6.843143   0.960463       -1  0.651375  \n",
       "52     0.646848          4.882684   0.691758       -1  0.665186  \n",
       "53     0.411207          7.027819   0.708383       -1  0.655811  \n",
       "54     0.082748          9.766615   0.820635       -1  0.656296  \n",
       "55     0.749578          4.929894   0.931666       -1  0.654717  \n",
       "56     0.534346          8.499107   0.965266       -1  0.668943  \n",
       "57     0.251442          3.747318   0.682891       -1  0.655579  \n",
       "58     0.831371          4.077772   0.926554       -1  0.665427  \n",
       "59     0.784668          7.393614   0.922018       -1  0.651956  \n",
       "60     0.091002          8.263970   0.818979       -1  0.651562  \n",
       "61     0.842091          8.689177   0.626494       -1  0.651227  \n",
       "62     0.128880          2.280458   0.660761       -1  0.660246  \n",
       "63     0.973110          7.019354   0.689540       -1  0.655341  \n",
       "64     0.355991          9.942173   0.651499       -1  0.666733  \n",
       "65     0.920872          1.610780   0.710751       -1  0.652180  \n",
       "66     0.618255          4.689136   0.785014       -1  0.659151  \n",
       "67     0.107172          5.474124   0.813047       -1  0.651117  \n",
       "68     0.327497          2.197621   0.956211       -1  0.665348  \n",
       "69     0.745439          5.316595   0.651032       -1  0.664966  \n",
       "70     0.605775          3.372268   0.640713       -1  0.652259  \n",
       "71     0.842314          1.516355   0.607297       -1  0.665925  \n",
       "72     0.975067          5.907488   0.889158       -1  0.651274  \n",
       "73     0.967994          8.109525   0.679803       -1  0.653948  \n",
       "74     0.932014          4.206422   0.837553       -1  0.649078  \n",
       "75     0.087868          2.388248   0.601084       -1  0.653373  \n",
       "76     0.965027          2.242972   0.892347       -1  0.657433  \n",
       "77     0.139772          8.952673   0.680651       -1  0.649722  \n",
       "78     0.877201          4.924451   0.926640       -1  0.652592  \n",
       "79     0.355362          4.914821   0.812743       -1  0.654073  \n",
       "80     0.693718          2.667308   0.951452       -1  0.655667  \n",
       "81     0.778572          2.431280   0.681818       -1  0.648488  \n",
       "82     0.397287          8.972954   0.659967       -1  0.652514  \n",
       "83     0.251799          4.456960   0.672639       -1  0.651058  \n",
       "84     0.139116          2.007946   0.702406       -1  0.658261  \n",
       "85     0.771247          4.744354   0.627569       -1  0.650128  \n",
       "86     0.165521          2.863209   0.934996       -1  0.649559  \n",
       "87     0.205580          8.877899   0.841489       -1  0.652467  \n",
       "88     0.918455          6.870682   0.613139       -1  0.649654  \n",
       "89     0.780496          3.775872   0.688039       -1  0.662032  \n",
       "90     0.535070          2.844629   0.719838       -1  0.652768  \n",
       "91     0.036550          5.720669   0.825936       -1  0.650194  \n",
       "92     0.765356          2.029815   0.733745       -1  0.652451  \n",
       "93     0.978931          9.397889   0.946798       -1  0.649384  \n",
       "94     0.799916          2.785447   0.861098       -1  0.661597  \n",
       "95     0.668085          3.049843   0.717259       -1  0.650875  \n",
       "96     0.583042          5.214246   0.957069       -1  0.661749  \n",
       "97     0.345192          8.738345   0.745103       -1  0.656279  \n",
       "98     0.985367          5.710655   0.672843       -1  0.649705  \n",
       "99     0.645054          5.217039   0.652910       -1  0.650048  \n",
       "100    0.498091          1.671754   0.762099       -1  0.649900  \n",
       "101    0.597413          7.005169   0.866015       -1  0.661565  \n",
       "102    0.166283          8.380336   0.633119       -1  0.655640  \n",
       "103    0.297122          6.664046   0.790420       -1  0.658948  \n",
       "104    0.841530          9.464078   0.982095       -1  0.659390  \n",
       "105    0.688402          5.348273   0.698561       -1  0.664165  \n",
       "106    0.091582         10.173136   0.654727       -1  0.659084  \n",
       "107    0.806561          7.587834   0.876911       -1  0.652952  \n",
       "108    0.044165          6.882638   0.999741       -1  0.659002  \n",
       "109    0.115282          5.217627   0.918137       -1  0.663365  \n",
       "110    0.364697          6.606772   0.792425       -1  0.653357  \n",
       "111    0.538401          7.616552   0.895142       -1  0.655563  \n",
       "112    0.874702          6.289371   0.975627       -1  0.665469  \n",
       "113    0.627505          9.736771   0.993633       -1  0.649569  \n",
       "114    0.297003          9.528218   0.886221       -1  0.650515  \n",
       "115    0.218254         10.499612   0.914538       -1  0.655713  \n",
       "116    0.743353          1.649864   0.760632       -1  0.653285  \n",
       "117    0.957270          9.262057   0.920308       -1  0.666262  \n",
       "118    0.758238          7.960206   0.661558       -1  0.647042  \n",
       "119    0.877461          3.656000   0.651806       -1  0.652707  \n",
       "120    0.086987          5.084532   0.749075       -1  0.649631  \n",
       "121    0.076131          9.512069   0.798059       -1  0.665231  \n",
       "122    0.267028          9.786300   0.918970       -1  0.657066  \n",
       "123    0.680141          7.186114   0.901087       -1  0.651206  \n",
       "124    0.743508          2.607599   0.927187       -1  0.649889  \n",
       "125    0.631814          3.598036   0.853602       -1  0.657647  \n",
       "126    0.392151          2.794973   0.901480       -1  0.651407  \n",
       "127    0.582170          4.458831   0.848366       -1  0.658653  \n",
       "128    0.024587          1.221236   0.729444       -1  0.653077  \n",
       "129    0.102194          9.542098   0.810810       -1  0.654699  \n",
       "130    0.950864          2.510579   0.772934       -1  0.650943  \n",
       "131    0.409334          9.940992   0.691982       -1  0.655792  \n",
       "132    0.159246          3.037012   0.636466       -1  0.655199  \n",
       "133    0.920848          7.166503   0.918615       -1  0.651799  \n",
       "134    0.064209          6.819714   0.707597       -1  0.653770  \n",
       "135    0.479939          7.000205   0.716665       -1  0.663678  \n",
       "136    0.242045         10.866626   0.656998       -1  0.656548  \n",
       "137    0.087866          4.506269   0.613281       -1  0.653262  \n",
       "138    0.200150          2.674826   0.641827       -1  0.652274  \n",
       "139    0.709061          9.709691   0.885635       -1  0.648729  \n",
       "140    0.783833          5.809038   0.616302       -1  0.651567  \n",
       "141    0.219214         10.758653   0.734758       -1  0.659035  \n",
       "142    0.228455         10.963339   0.989917       -1  0.653505  \n",
       "143    0.462623          9.682725   0.890868       -1  0.665478  \n",
       "144    0.588058          1.293944   0.758206       -1  0.655199  \n",
       "145    0.359151          4.838472   0.786325       -1  0.652634  \n",
       "146    0.372849          6.126152   0.765212       -1  0.655255  \n",
       "147    0.095262          7.521232   0.724859       -1  0.655687  \n",
       "148    0.453790          3.360505   0.629399       -1  0.650601  \n",
       "149    0.617145          8.067772   0.666817       -1  0.649380  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgmbres = pd.DataFrame([{**entry['params'], 'score': entry['score']} for entry in wrapperLGBM.get_results()])\n",
    "lgmbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-14T07:44:58.497395Z",
     "iopub.status.idle": "2024-12-14T07:44:58.497775Z",
     "shell.execute_reply": "2024-12-14T07:44:58.497608Z",
     "shell.execute_reply.started": "2024-12-14T07:44:58.497591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CatBoost Regressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "param_distributions = {\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'depth': randint(3, 12),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bylevel': uniform(0.6, 0.4),\n",
    "    'min_data_in_leaf': randint(1, 20),\n",
    "    'max_ctr_complexity': randint(1, 10),\n",
    "    'grow_policy': [\"Lossguide\"],\n",
    "    'bootstrap_type':['Bernoulli'], \n",
    "    'leaf_estimation_method':['Gradient']\n",
    "}\n",
    "# Initialize CatBoost Regressor\n",
    "catboost_regressor = CatBoostRegressor(random_state=42, silent=True)\n",
    "\n",
    "# Set up RandomizedSearchCV with 50 iterations and KFold cross-validation\n",
    "wrappercat = CustomRandomizedSearchCV(\n",
    "    estimator=catboost_regressor,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=scorer,\n",
    "    cv=kf,\n",
    "    n_iter=150,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the wrapper with X and y_full\n",
    "wrappercat.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", wrappercat.best_params_)\n",
    "print(\"Best Score:\", wrappercat.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
